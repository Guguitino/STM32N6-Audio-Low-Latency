/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    network.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "e619e8606099384540d70eeaaa8091752b1bebe9"
 * GIT_BRANCH      "STAI-2.2"
 * GIT_DESCRIPTION "atonn-v1.1.1-14-ge619e860"
 *
 * Command Line options:
 * --load-mdesc-file = "/home/adamg/STApps/GS_Audio_N6/STM32N6-Audio-Low-Latency/Projects/X-CUBE-AI/models/stm32n6"
 * --load-mpool-file = "/home/adamg/STApps/GS_Audio_N6/STM32N6-Audio-Low-Latency/Projects/X-CUBE-AI/models/stm32n6"
 * --cache-maintenance = true
 * --native-float = true
 * --json-quant-file = "/home/adamg/STApps/ModelZOO/stm32ai-modelzoo-services/speech_enhancement/src/experiment_outputs/2025_10_03_13_57_33/trained_model_OE_3_3_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --csv-file = "network"
 * --output-info-file = "c_info"
 * --onnx-input = "/home/adamg/STApps/ModelZOO/stm32ai-modelzoo-services/speech_enhancement/src/experiment_outputs/2025_10_03_13_57_33/trained_model_OE_3_3_0.onnx"
 * --out-dir-prefix = "/home/adamg/STApps/ModelZOO/stm32ai-modelzoo-services/speech_enhancement/src/experiment_outputs/2025_10_03_13_57_33/neural_art__network/"
 * --all-buffers-info = true
 * --mvei = true
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 1 || LL_ATON_VERSION_DEV != 14
#  error "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 4 is ? */
/* index=4 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=16777216 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 5 is 4.05 MB */
/* index=5 file postfix=xSPI2 name=octoFlash offset=0x70180000  absolute_mode size=66060288 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 0 is 7.04 KB */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 1 is ? */
/* index=1 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 2 is ? */
/* index=2 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */
/* global pool 3 is ? */
/* index=3 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

bool LL_ATON_EC_Network_Init_Default(void)
{
  return true;
}

bool LL_ATON_EC_Inference_Init_Default(void)
{
  return true;
}

/* scheduling epoch=0    nodes=21  ------------------------------------------------------------------- */

/* scheduling epoch=1    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_2 */
  Conv_sw_info conv1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 257,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 1028,
    .general.input.stride.w = 1028,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 257,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 1028,
    .weights.stride.h = 1028,
    .weights.stride.w = 1028,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 0))) /* Equivalent hex address = 0x70180000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4235264))) /* Equivalent hex address = 0x7058a000UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_2 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 2080);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=4    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_4(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_4 */
  Activ_sw_info activ2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_4 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 2080);

}


/* scheduling epoch=5    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=6    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_6(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_6 */
  Conv_sw_info conv3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 1,
    .weights.dim.num_elem = 1536,
    .weights.stride.b = 12,
    .weights.stride.h = 4,
    .weights.stride.w = 4,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4210688))) /* Equivalent hex address = 0x70584000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4243456))) /* Equivalent hex address = 0x7058c000UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3088))) /* Equivalent hex address = 0x34350c10UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 512,
    .pads = {1, 0, 1, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_6 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 2080);

}


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=8    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_8(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3104))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_8 */
  Activ_sw_info activ4_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3088))) /* Equivalent hex address = 0x34350c10UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3088))) /* Equivalent hex address = 0x34350c10UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_8 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ4_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3072))) /* Equivalent hex address = 0x34350c00UL */, 2080);

}


/* scheduling epoch=9    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=10   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_10(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_10 */
  Conv_sw_info conv5_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 3088))) /* Equivalent hex address = 0x34350c10UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 257,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 512,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 2048,
    .weights.stride.h = 2048,
    .weights.stride.w = 2048,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 2105344))) /* Equivalent hex address = 0x70382000UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 257,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 1028,
    .general.output.stride.w = 1028,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_10 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv5_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 1056);

}


/* scheduling epoch=11   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=12   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_12(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Add node=Add_12 */
  Arith_sw_info arith6_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 257,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 257,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 257,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_12 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith6_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */, 1056);

}


/* scheduling epoch=13   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=14   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_14(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_14 */
  Conv_sw_info conv7_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 257,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 1028,
    .general.input.stride.w = 1028,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 257,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 1028,
    .weights.stride.h = 1028,
    .weights.stride.w = 1028,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 526336))) /* Equivalent hex address = 0x70200800UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4237312))) /* Equivalent hex address = 0x7058a800UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_14 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv7_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=15   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=16   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_16(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_16 */
  Activ_sw_info activ8_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_16 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ8_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=18   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_18(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_18 */
  Conv_sw_info conv9_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 1,
    .weights.dim.num_elem = 1536,
    .weights.stride.b = 12,
    .weights.stride.h = 4,
    .weights.stride.w = 4,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4216832))) /* Equivalent hex address = 0x70585800UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4245504))) /* Equivalent hex address = 0x7058c800UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 512,
    .pads = {2, 0, 2, 0},
    .strides = {1, 1},
    .dilations = {2, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_18 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv9_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=19   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=20   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_20(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_20 */
  Activ_sw_info activ10_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_20 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ10_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=21   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_22 */
  Conv_sw_info conv11_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 257,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 512,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 2048,
    .weights.stride.h = 2048,
    .weights.stride.w = 2048,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 2631680))) /* Equivalent hex address = 0x70402800UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 257,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 1028,
    .general.output.stride.w = 1028,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_22 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv11_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 1056);

}


/* scheduling epoch=23   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=24   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_24(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6144))) /* Equivalent hex address = 0x34351800UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Add node=Add_24 */
  Arith_sw_info arith12_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 257,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 257,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 257,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5136))) /* Equivalent hex address = 0x34351410UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_24 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith12_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 1056);

}


/* scheduling epoch=25   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=26   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_26(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_26 */
  Conv_sw_info conv13_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 257,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 1028,
    .general.input.stride.w = 1028,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5136))) /* Equivalent hex address = 0x34351410UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 257,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 1028,
    .weights.stride.h = 1028,
    .weights.stride.w = 1028,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 1052672))) /* Equivalent hex address = 0x70281000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4239360))) /* Equivalent hex address = 0x7058b000UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_26 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv13_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=27   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=28   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_28(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_28 */
  Activ_sw_info activ14_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_28 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ14_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=29   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=30   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_30(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_30 */
  Conv_sw_info conv15_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 1,
    .weights.dim.num_elem = 1536,
    .weights.stride.b = 12,
    .weights.stride.h = 4,
    .weights.stride.w = 4,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4222976))) /* Equivalent hex address = 0x70587000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4247552))) /* Equivalent hex address = 0x7058d000UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 512,
    .pads = {1, 0, 1, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_30 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv15_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=31   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=32   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_32(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_32 */
  Activ_sw_info activ16_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_32 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ16_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=33   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=34   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_34(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5120))) /* Equivalent hex address = 0x34351400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_34 */
  Conv_sw_info conv17_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 257,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 512,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 2048,
    .weights.stride.h = 2048,
    .weights.stride.w = 2048,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 3158016))) /* Equivalent hex address = 0x70483000UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 257,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 1028,
    .general.output.stride.w = 1028,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_34 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv17_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */, 1056);

}


/* scheduling epoch=35   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=36   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_36(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 7200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 7232))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 7200))) /* Equivalent hex address = 0x34351c20UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Add node=Add_36 */
  Arith_sw_info arith18_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 257,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) /* Equivalent hex address = 0x34351000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 257,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 5136))) /* Equivalent hex address = 0x34351410UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 257,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) /* Equivalent hex address = 0x34351820UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_36 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith18_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 7232))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) /* Equivalent hex address = 0x34351820UL */, 1056);

}


/* scheduling epoch=37   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=38   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_38(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_38 */
  Conv_sw_info conv19_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 257,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 1028,
    .general.input.stride.w = 1028,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) /* Equivalent hex address = 0x34351820UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 257,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 1028,
    .weights.stride.h = 1028,
    .weights.stride.w = 1028,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 1579008))) /* Equivalent hex address = 0x70301800UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4241408))) /* Equivalent hex address = 0x7058b800UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_38 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv19_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=39   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=40   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_40(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_40 */
  Activ_sw_info activ20_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_40 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ20_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 2048);

}


/* scheduling epoch=41   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=42   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_42(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_42 */
  Conv_sw_info conv21_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 512,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 1,
    .weights.dim.num_elem = 1536,
    .weights.stride.b = 12,
    .weights.stride.h = 4,
    .weights.stride.w = 4,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4229120))) /* Equivalent hex address = 0x70588800UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 4249600))) /* Equivalent hex address = 0x7058d800UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 512,
    .pads = {2, 0, 2, 0},
    .strides = {1, 1},
    .dilations = {2, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_42 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv21_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=43   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=44   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_44(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Relu node=Relu_44 */
  Activ_sw_info activ22_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 512,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 512,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_44 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ22_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 4096))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 2048);

}


/* scheduling epoch=45   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=46   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_46(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_46 */
  Conv_sw_info conv23_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 2048,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 257,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 512,
    .weights.dim.num_elem = 131584,
    .weights.stride.b = 2048,
    .weights.stride.h = 2048,
    .weights.stride.w = 2048,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70180000UL + 3684352))) /* Equivalent hex address = 0x70503800UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 257,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 1028,
    .general.output.stride.w = 1028,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_46 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv23_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 1056);

}


/* scheduling epoch=47   nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=48   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_48(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Add node=Add_48 */
  Arith_sw_info arith24_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 257,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 257,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 6176))) /* Equivalent hex address = 0x34351820UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 257,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_48 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith24_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 1056);

}


/* scheduling epoch=49   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_49(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2048))) /* Equivalent hex address = 0x34350800UL */, 32);


/* Unit= 25 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_49 */
  Activ_sw_info activ25_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 257,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 257,
    .general.input.stride.b = 1028,
    .general.input.stride.h = 4,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 257,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 257,
    .general.output.stride.b = 1028,
    .general.output.stride.h = 4,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1040))) /* Equivalent hex address = 0x34350410UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_49 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ25_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 2080))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 1024))) /* Equivalent hex address = 0x34350400UL */, 1056);

}


/* scheduling epoch=50   nodes=1   ------------------------------------------------------------------- */

/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_Default(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_4,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 4,
      .last_epoch_num = 4,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_6,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 6,
      .last_epoch_num = 6,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_8,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 8,
      .last_epoch_num = 8,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_10,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 10,
      .last_epoch_num = 10,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_12,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 12,
      .last_epoch_num = 12,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_14,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_16,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_18,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 18,
      .last_epoch_num = 18,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_20,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 20,
      .last_epoch_num = 20,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_24,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 24,
      .last_epoch_num = 24,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_26,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 26,
      .last_epoch_num = 26,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_28,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 28,
      .last_epoch_num = 28,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_30,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 30,
      .last_epoch_num = 30,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_32,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 32,
      .last_epoch_num = 32,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_34,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 34,
      .last_epoch_num = 34,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_36,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 36,
      .last_epoch_num = 36,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_38,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 38,
      .last_epoch_num = 38,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_40,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 40,
      .last_epoch_num = 40,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_42,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 42,
      .last_epoch_num = 42,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_44,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 44,
      .last_epoch_num = 44,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_46,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 46,
      .last_epoch_num = 46,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_48,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 48,
      .last_epoch_num = 48,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_49,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 49,
      .last_epoch_num = 49,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_257_1[] = { 1, 257, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_257_1[] = { 1, 257, 1 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_512_257_1_1[] = { 512, 1, 1, 257 };
  static const uint32_t buff_info__mem_shape_F_512_257_1_1[] = { 512, 257, 1, 1 };
  static const uint32_t buff_info__shape_512[] = { 1, 1, 512, 1 };
  static const uint32_t buff_info__mem_shape_U_512[] = { 512 };
  static const uint32_t buff_info__shape_512_1_3_1[] = { 512, 3, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_512_1_3_1[] = { 512, 1, 3, 1 };
  static const uint32_t buff_info__shape_257_512_1_1[] = { 257, 1, 1, 512 };
  static const uint32_t buff_info__mem_shape_F_257_512_1_1[] = { 257, 512, 1, 1 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_0_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Conv2D_2_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 0,
      .offset_end = 526336,
      .offset_limit = 526400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_512_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_257_1_1,
    },
    {
      .name = "Conv2D_2_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4235264,
      .offset_end = 4237312,
      .offset_limit = 4237376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_6_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4210688,
      .offset_end = 4216832,
      .offset_limit = 4216896,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_512_1_3_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_1_3_1,
    },
    {
      .name = "Conv2D_6_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4243456,
      .offset_end = 4245504,
      .offset_limit = 4245568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_10_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 2105344,
      .offset_end = 2631680,
      .offset_limit = 2631744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_257_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_257_512_1_1,
    },
    {
      .name = "Conv2D_14_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 526336,
      .offset_end = 1052672,
      .offset_limit = 1052736,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_512_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_257_1_1,
    },
    {
      .name = "Conv2D_14_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4237312,
      .offset_end = 4239360,
      .offset_limit = 4239424,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_18_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4216832,
      .offset_end = 4222976,
      .offset_limit = 4223040,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_512_1_3_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_1_3_1,
    },
    {
      .name = "Conv2D_18_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4245504,
      .offset_end = 4247552,
      .offset_limit = 4247616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_22_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 2631680,
      .offset_end = 3158016,
      .offset_limit = 3158080,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_257_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_257_512_1_1,
    },
    {
      .name = "Conv2D_26_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 1052672,
      .offset_end = 1579008,
      .offset_limit = 1579072,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_512_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_257_1_1,
    },
    {
      .name = "Conv2D_26_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4239360,
      .offset_end = 4241408,
      .offset_limit = 4241472,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_30_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4222976,
      .offset_end = 4229120,
      .offset_limit = 4229184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_512_1_3_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_1_3_1,
    },
    {
      .name = "Conv2D_30_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4247552,
      .offset_end = 4249600,
      .offset_limit = 4249664,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_34_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 3158016,
      .offset_end = 3684352,
      .offset_limit = 3684416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_257_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_257_512_1_1,
    },
    {
      .name = "Conv2D_38_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 1579008,
      .offset_end = 2105344,
      .offset_limit = 2105408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_512_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_257_1_1,
    },
    {
      .name = "Conv2D_38_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4241408,
      .offset_end = 4243456,
      .offset_limit = 4243520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_42_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4229120,
      .offset_end = 4235264,
      .offset_limit = 4235328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_512_1_3_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512_1_3_1,
    },
    {
      .name = "Conv2D_42_bias",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 4249600,
      .offset_end = 4251648,
      .offset_limit = 4251712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_46_weights",
      .addr_base = {(unsigned char *)(0x70180000UL) /* Equivalent hex address = 0x70180000UL */},
      .offset_start = 3684352,
      .offset_end = 4210688,
      .offset_limit = 4210752,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_257_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_257_512_1_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_257_1[] = { 1, 257, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_257_1[] = { 1, 257, 1 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Sigmoid_49_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 2068,
      .offset_limit = 2132,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 49,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_257_1_1[] = { 1, 1, 1, 257 };
  static const uint32_t buff_info__mem_shape_F_1_257_1_1[] = { 1, 257, 1, 1 };
  static const uint32_t buff_info__shape_1_512_1_1[] = { 1, 1, 1, 512 };
  static const uint32_t buff_info__mem_shape_F_1_512_1_1[] = { 1, 512, 1, 1 };
  static const uint32_t buff_info__shape_1_512_1[] = { 1, 512, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_512_1[] = { 1, 512, 1 };
  static const uint32_t buff_info__shape_1_257_1[] = { 1, 257, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_257_1[] = { 1, 257, 1 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Reshape_1_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Conv2D_2_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 3088,
      .offset_limit = 3152,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_3_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 3088,
      .offset_limit = 3152,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_4_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 3088,
      .offset_limit = 3152,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_5_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 3088,
      .offset_limit = 3152,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_6_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 3088,
      .offset_end = 5136,
      .offset_limit = 5200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_7_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 3088,
      .offset_end = 5136,
      .offset_limit = 5200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_8_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 3088,
      .offset_end = 5136,
      .offset_limit = 5200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_9_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 3088,
      .offset_end = 5136,
      .offset_limit = 5200,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_10_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 2068,
      .offset_limit = 2132,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Reshape_11_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 2068,
      .offset_limit = 2132,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Add_12_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 4096,
      .offset_end = 5124,
      .offset_limit = 5188,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Reshape_13_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 4096,
      .offset_end = 5124,
      .offset_limit = 5188,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Conv2D_14_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_15_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_16_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_17_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_18_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_19_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_20_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_21_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_22_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Reshape_23_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Add_24_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 5136,
      .offset_end = 6164,
      .offset_limit = 6228,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Reshape_25_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 5136,
      .offset_end = 6164,
      .offset_limit = 6228,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 25,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Conv2D_26_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_27_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 27,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_28_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_29_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 29,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_30_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 30,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_31_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_32_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_33_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_34_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 4096,
      .offset_end = 5124,
      .offset_limit = 5188,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 34,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Reshape_35_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 4096,
      .offset_end = 5124,
      .offset_limit = 5188,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Add_36_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 6176,
      .offset_end = 7204,
      .offset_limit = 7268,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Reshape_37_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 6176,
      .offset_end = 7204,
      .offset_limit = 7268,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 37,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Conv2D_38_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_39_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 39,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_40_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 40,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_41_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 2048,
      .offset_limit = 2112,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_42_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 42,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Reshape_43_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Relu_44_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 44,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1,
    },
    {
      .name = "Reshape_45_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 2048,
      .offset_end = 4096,
      .offset_limit = 4160,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 45,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_1_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_1_1,
    },
    {
      .name = "Conv2D_46_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "Reshape_47_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 1028,
      .offset_limit = 1092,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 47,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = "Add_48_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 1040,
      .offset_end = 2068,
      .offset_limit = 2132,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_257_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

